{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi-11nav/Text-Emotion-Detection/blob/main/Text_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zofTXg4alSny"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importing the necessary libraries \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WhgKucVla2i",
        "outputId": "86b4cfa9-b952-4c70-bf28-91a87f2ad469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Text-Emotion-Detection'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 60 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ]
        }
      ],
      "source": [
        "# Cloning the github repository \n",
        "\n",
        "!git clone https://github.com/abhi-11nav/Text-Emotion-Detection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hKbC0BeildcI"
      },
      "outputs": [],
      "source": [
        "# Importing data\n",
        "\n",
        "data = pd.read_csv(\"/content/Text-Emotion-Detection/tweet_emotions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Yu8taALUlofD",
        "outputId": "64726e0f-6345-4c40-d3cc-aa2481ae1d49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     tweet_id   sentiment                                            content\n",
              "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
              "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
              "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a37cf662-00eb-4e72-9b7f-fa906d99fc96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968416</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a37cf662-00eb-4e72-9b7f-fa906d99fc96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a37cf662-00eb-4e72-9b7f-fa906d99fc96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a37cf662-00eb-4e72-9b7f-fa906d99fc96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funeral ceremony...gloomy friday..."
      ],
      "metadata": {
        "id": "OLCfmZ18pXYF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TIUmyCIAmA-_"
      },
      "outputs": [],
      "source": [
        "# Let us drop the tweet id\n",
        "\n",
        "data.drop(\"tweet_id\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uIDRz4D-mJXk",
        "outputId": "e5259460-6c77-4cdd-d537-ff7f5db326be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2     sadness                Funeral ceremony...gloomy friday...\n",
              "3  enthusiasm               wants to hang out with friends SOON!\n",
              "4     neutral  @dannycastillo We want to trade with someone w..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d154fd48-b210-48a1-9c11-ca57903a5163\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d154fd48-b210-48a1-9c11-ca57903a5163')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d154fd48-b210-48a1-9c11-ca57903a5163 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d154fd48-b210-48a1-9c11-ca57903a5163');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeEFLErwmZxK",
        "outputId": "c23c0dd6-2968-4e4c-f582-72496902204e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment    False\n",
              "content      False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Let us check if the tweet has any missing values \n",
        "\n",
        "data.isna().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syZBYwVdmubC"
      },
      "source": [
        "No missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLiURfjqmszA",
        "outputId": "45abc615-5a1f-422a-db6c-c889dd36a563"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral       8638\n",
              "worry         8459\n",
              "happiness     5209\n",
              "sadness       5165\n",
              "love          3842\n",
              "surprise      2187\n",
              "fun           1776\n",
              "relief        1526\n",
              "hate          1323\n",
              "empty          827\n",
              "enthusiasm     759\n",
              "boredom        179\n",
              "anger          110\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Let us check the number of categories in sentiment variable\n",
        "\n",
        "data['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRYV6lDBnsw7"
      },
      "source": [
        "Since the data is imbalanced, we'll be deadling with it "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Imbalance"
      ],
      "metadata": {
        "id": "FEMWU1Y9GEbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminating the last two categories of sentiment as they are least represented. "
      ],
      "metadata": {
        "id": "lu2uF4piNa6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the last two samples\n",
        "\n",
        "# Appending indexes to remove\n",
        "indexes_to_remove = []\n",
        "\n",
        "\n",
        "for index in data[data['sentiment']==\"boredom\"].index:\n",
        "  indexes_to_remove.append(index)\n",
        "\n",
        "for index in data[data['sentiment']==\"anger\"].index:\n",
        "  indexes_to_remove.append(index)"
      ],
      "metadata": {
        "id": "uGGCatHTGE8P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(indexes_to_remove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAgIIK9IOnll",
        "outputId": "c84dc27a-a52e-48f0-8533-6d4fb30c383c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "289"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(indexes_to_remove, inplace=True, axis=0)"
      ],
      "metadata": {
        "id": "GaacF126Ono5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPl5Fzn4GE-x",
        "outputId": "86c1b19e-0183-40d6-b9d8-cb06fce6a324"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral       8638\n",
              "worry         8459\n",
              "happiness     5209\n",
              "sadness       5165\n",
              "love          3842\n",
              "surprise      2187\n",
              "fun           1776\n",
              "relief        1526\n",
              "hate          1323\n",
              "empty          827\n",
              "enthusiasm     759\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label for label in data[\"sentiment\"].unique()]"
      ],
      "metadata": {
        "id": "7hl9qozlNPEr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df = pd.DataFrame()\n",
        "\n",
        "for label in labels: \n",
        "  balanced_df = pd.concat([data[data[\"sentiment\"]==label].sample(759),balanced_df], axis=0)"
      ],
      "metadata": {
        "id": "bOK_Yc3lGFBK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkiL8HfSGFGF",
        "outputId": "740b9f34-3783-4844-e07f-21ba11d6fa5f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "relief        759\n",
              "happiness     759\n",
              "hate          759\n",
              "fun           759\n",
              "love          759\n",
              "surprise      759\n",
              "worry         759\n",
              "neutral       759\n",
              "enthusiasm    759\n",
              "sadness       759\n",
              "empty         759\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now we have a balanced dataset"
      ],
      "metadata": {
        "id": "l65X2l8lXPjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffling samples and resetting indexes\n",
        "\n",
        "balanced_df = balanced_df.sample(len(balanced_df))"
      ],
      "metadata": {
        "id": "KhDwpJKWGFJO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "OLRi0Mj0YwGy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xfkwmfzPYwCW",
        "outputId": "9ff454ec-27c0-4f08-d721-e04304b15046"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index   sentiment                                            content\n",
              "0   5573        hate  @michelleflores Michelle, I slept for 11 hours...\n",
              "1  26889         fun  @YuraF Nope wasn't kidding at all.  Sometimes ...\n",
              "2  24974  enthusiasm                    @NEENZ Bye!! Great meeting you!\n",
              "3    368        hate  RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED...\n",
              "4  25852    surprise  @iyaitssuzanne ohh yeh , but he was on sexy me..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-db25f55a-8a6b-4075-ad1f-c7f610cfc054\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5573</td>\n",
              "      <td>hate</td>\n",
              "      <td>@michelleflores Michelle, I slept for 11 hours...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26889</td>\n",
              "      <td>fun</td>\n",
              "      <td>@YuraF Nope wasn't kidding at all.  Sometimes ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24974</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>@NEENZ Bye!! Great meeting you!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>368</td>\n",
              "      <td>hate</td>\n",
              "      <td>RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25852</td>\n",
              "      <td>surprise</td>\n",
              "      <td>@iyaitssuzanne ohh yeh , but he was on sexy me...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db25f55a-8a6b-4075-ad1f-c7f610cfc054')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db25f55a-8a6b-4075-ad1f-c7f610cfc054 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db25f55a-8a6b-4075-ad1f-c7f610cfc054');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.drop(\"index\", inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "RdsGAuZaYs49"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the name of the data frame\n",
        "\n",
        "data = balanced_df"
      ],
      "metadata": {
        "id": "zeAMfYLJaFRW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "4ITV_GjAnT70",
        "outputId": "6b333bbe-0b72-43f8-e495-fa50a108f012"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@michelleflores Michelle, I slept for 11 hours last night. I'm still stick with this fever.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Let us look at the sentences\n",
        "\n",
        "data['content'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "LqKVBAqVoMhn",
        "outputId": "514665f5-bc6d-4186-9097-c7585c62d1cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@YuraF Nope wasn't kidding at all.  Sometimes I think of you as Forest Gump (during his running years).\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "data['content'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9VKL5TVpDiw"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMD6rP6Hr_xt",
        "outputId": "a19dea47-ca2d-482b-f70d-4018c100da8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries\n",
        "\n",
        "import re \n",
        "\n",
        "import nltk \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hG_T2DWSo925"
      },
      "outputs": [],
      "source": [
        "def text_preprocess(dataset,list_name):\n",
        "  \n",
        "  for i in range(dataset.shape[0]):\n",
        "    list_name.append(re.sub('[^a-zA-Z]',' ',str(dataset.iloc[i,1])))\n",
        "\n",
        "  print(\"Number and other symbols eliminated from the text\")\n",
        "\n",
        "  # String spacing \n",
        "  for x in range(len(list_name)):\n",
        "    list_name[x] = \" \".join(y for y in str(list_name[x]).split()).lower()\n",
        "\n",
        "  print(\"Text reorganized and converted to small letter\")\n",
        "  \n",
        "  for index in range(len(list_name)):\n",
        "    temp_list= []\n",
        "    # Lemmatization\n",
        "    for word in list_name[index].split():\n",
        "      if word not in stopwords.words('english'):\n",
        "        temp_list.append(word)\n",
        "    list_name[index] = \" \".join(lemmatizer.lemmatize(words) for words in temp_list )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiOLAlRoo952",
        "outputId": "6fa318c9-fb56-4018-8562-5fd16a09e748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number and other symbols eliminated from the text\n",
            "Text reorganized and converted to small letter\n"
          ]
        }
      ],
      "source": [
        "sentences = []\n",
        "\n",
        "text_preprocess(data,sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_data = pd.concat([pd.DataFrame(np.array(sentences), columns=[\"Content\"]), data['sentiment']], axis=1)"
      ],
      "metadata": {
        "id": "rFIZ0Mdz5f6s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "940QFH6M-rOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "TcbulVrDFjHF",
        "outputId": "bb1068b9-0dad-4dd6-fd9e-27e448591cc1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Content   sentiment\n",
              "0  michelleflores michelle slept hour last night ...        hate\n",
              "1  yuraf nope kidding sometimes think forest gump...         fun\n",
              "2                            neenz bye great meeting  enthusiasm\n",
              "3  ratt rocked nashville tonite one thing sucked ...        hate\n",
              "4                iyaitssuzanne ohh yeh sexy men okay    surprise"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-521e54e9-c706-4175-af4d-81d34161c1d3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>michelleflores michelle slept hour last night ...</td>\n",
              "      <td>hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>yuraf nope kidding sometimes think forest gump...</td>\n",
              "      <td>fun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neenz bye great meeting</td>\n",
              "      <td>enthusiasm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ratt rocked nashville tonite one thing sucked ...</td>\n",
              "      <td>hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>iyaitssuzanne ohh yeh sexy men okay</td>\n",
              "      <td>surprise</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-521e54e9-c706-4175-af4d-81d34161c1d3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-521e54e9-c706-4175-af4d-81d34161c1d3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-521e54e9-c706-4175-af4d-81d34161c1d3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_6z1WOXKbW5"
      },
      "source": [
        "Text preprocessing done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One hot Encoding and Padding sequences \n",
        "\n",
        "p_data is the data"
      ],
      "metadata": {
        "id": "JgO-k3_Ab_7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference : https://phdstatsphys.wordpress.com/2018/12/27/word2vec-how-to-train-and-update-it/"
      ],
      "metadata": {
        "id": "V3E4xpOEmdNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "\n",
        "import gensim.downloader as api\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca4Mnx1Db88W",
        "outputId": "162a642f-f765-483f-e8d6-b29027740aa3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_word2vec = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9XbegjMb9Ad",
        "outputId": "b6b5db25-e032-4da5-b3aa-978326272da3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -c 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'"
      ],
      "metadata": {
        "id": "OR2fLgxbm1f4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = gensim.models.Word2Vec.load_word2vec_format('path', binary=True)"
      ],
      "metadata": {
        "id": "7anbhNr4EgJ-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = google_word2vec"
      ],
      "metadata": {
        "id": "82fiZOI7EgMx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "VqwUNsA3EgSU",
        "outputId": "c47281f2-2b19-4b71-fcb3-92d51209f974"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'man'in wv.vocab"
      ],
      "metadata": {
        "id": "gybAuvUab9j6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2bbbd2-2e02-45d2-c1e0-d83d8891acf8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for sent in sentences:\n",
        "  corpus.append(sent.split())"
      ],
      "metadata": {
        "id": "d-au0BOCb9p2"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec(sentence, vector_list,set_index):\n",
        "  for index in range(len(sentence)):\n",
        "    if sentence[index] in wv.vocab:\n",
        "      sentence[index] = wv[sentence[index]]\n",
        "  \n",
        "  \n",
        "  for vectors in sentence:\n",
        "    if type(vectors)!= np.ndarray:\n",
        "      sentence.remove(vectors)\n",
        "      set_index.add(index)\n",
        "  \n",
        "  vector_list.append(sentence)"
      ],
      "metadata": {
        "id": "C2pFKwZRb9td"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = set()\n",
        "vectors = []\n",
        "\n",
        "\n",
        "for sent in corpus:\n",
        "  word2vec(sent, vectors, indexes)"
      ],
      "metadata": {
        "id": "_R_W-8-pb9xb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c789bd8-ec14-4f52-fc46-63c88ce6201c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-6654347b67d5>:9: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  sentence.remove(vectors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have vectors stored in **vectors**"
      ],
      "metadata": {
        "id": "-s6ApnHXN9Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors[0] = np.array(vectors[0])"
      ],
      "metadata": {
        "id": "HDpse5X6b90j"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdxzCASuQIgr",
        "outputId": "d5c12ab5-26b8-45f7-ee6a-391833d24a2c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for v in range(len(vectors)):\n",
        "  vectors[v] = np.array(vectors[v])"
      ],
      "metadata": {
        "id": "QNj_3M-qb930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e7a123-dc3e-4292-d519-1607acb48112"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-127-ec7e6fef3696>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  vectors[v] = np.array(vectors[v])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_lLBl7oLPQ41"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(vectors, encodings,test_size=0.15)"
      ],
      "metadata": {
        "id": "STML8gOLPQ8o"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSWoIst-PQ_H",
        "outputId": "73d5ec59-41a3-457c-b0b1-d58ca29b3dc7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7096,)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTqNGXjDPRBI",
        "outputId": "98cdfa28-aca2-4816-d107-fb7f58c979b8"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1253,)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdhVr7LFPRDR",
        "outputId": "8942d528-3603-480e-b683-7d16f5b65ae2"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7096,)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CngGjctyPRHJ",
        "outputId": "e404c91b-9a07-44c4-ed70-4201d73c9c7b"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1253,)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors[5][0] == np.ndarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkEEv-aPPRJJ",
        "outputId": "75558cd8-6a44-46fe-efb2-68e77101f357"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "bDCcoBJ-cGEu"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = ['a','b','c','d','e','f']"
      ],
      "metadata": {
        "id": "v5YYMmzUaKzt"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_sequences(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "m8spyOmdaK2S",
        "outputId": "91d2450a-b542-440c-920e-7e5488c9c2c3"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-77314b5b666d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m     \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m       raise ValueError(f'Shape of sample {trunc.shape[1:]} of sequence at '\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'a'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNE8xGUDaK6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymMTgmajaK8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8JMp7BCaLBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dndtBo0EUtUN"
      },
      "source": [
        "Converting text to vectors \n",
        "\n",
        "Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz-Kj1N0Uwgr",
        "outputId": "48e0e05e-16be-4e5d-f011-11352ece7cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04FTcZBfFa6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5bKDKJyWx50",
        "outputId": "63729c56-53b4-40b4-ea82-e10a35c5962c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8349  length of sentences\n"
          ]
        }
      ],
      "source": [
        "# Words list\n",
        "\n",
        "words_list = []\n",
        "\n",
        "# looping through to append words\n",
        "for index in range(len(sentences)):\n",
        "  words_list.append(nltk.word_tokenize(sentences[index]))\n",
        "\n",
        "print(len(words_list),\" length of sentences\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty_lists = []\n",
        "\n",
        "for i,wl in enumerate(words_list):\n",
        "  if not wl:\n",
        "    empty_lists.append(i)\n",
        "\n",
        "print(\"The number of empty lists are: \", len(empty_lists))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5cz9ytD_0x7",
        "outputId": "dbefb82a-13b5-400c-d6ad-922b002b3a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of empty lists are:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are 21 empty lists. We will combine them with the labels and drop the 21 rows"
      ],
      "metadata": {
        "id": "83iHg3J0Agv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us combine the dataset and get rid of any null values that may have occured after preprocessing\n",
        "\n",
        "preprocessed_data = pd.concat([pd.DataFrame(np.array(words_list)),pd.DataFrame(data['sentiment'])], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJUqQDY6AUY1",
        "outputId": "419bf799-66f8-400c-db83-ed16e814497b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for null values \n",
        "\n",
        "preprocessed_data.isna().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f6mabsOAz2D",
        "outputId": "0718abd3-3493-4934-ec93-92e521e6a92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            False\n",
              "sentiment    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have empty lists that we have to get rid of and we have the indexes of those lists store in empty_lists list\n",
        "\n",
        "# Verifying elemnts from the list\n",
        "\n",
        "for indexes in empty_lists:\n",
        "  print(preprocessed_data.iloc[indexes,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHC82BgeAz5f",
        "outputId": "7bd5c715-96d0-4a3b-9346-0449ee43af98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go, our empty lists. "
      ],
      "metadata": {
        "id": "MY8pOwQhC0SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data.drop(empty_lists, axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "1IarhUKTC5Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_lists = [lists for lists in preprocessed_data.iloc[:,0]]"
      ],
      "metadata": {
        "id": "9Re36EArC5Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1zdnQ9KUwkB"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec(words_list, window=5, min_count = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifTQTZJnUwmX",
        "outputId": "5908d1a9-5d48-4dd1-b62a-a9aa577e3c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8347 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "100%|██████████| 8347/8347 [00:01<00:00, 4737.44it/s]\n"
          ]
        }
      ],
      "source": [
        "# Empty list \n",
        "X = []\n",
        "\n",
        "# Looping though words\n",
        "for words in tqdm(word_lists):\n",
        "  X.append(np.mean([model.wv[word] for word in words if word in model.wv.index2word], axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coverting them to arrays\n",
        "\n",
        "X = np.array(X)\n",
        "y = preprocessed_data['sentiment']"
      ],
      "metadata": {
        "id": "6k7AnxBbQuuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9421a7f3-f11c-4193-a99d-976e4d6da076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = p_data['sentiment']"
      ],
      "metadata": {
        "id": "eH6OFX88PW-9"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "corresponding_num = []\n",
        "\n",
        "for ind,lab in enumerate(y.unique()):\n",
        "  labels.append(lab)\n",
        "  corresponding_num.append(ind)"
      ],
      "metadata": {
        "id": "DCShbT5-Eigt"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = [val for val in y]"
      ],
      "metadata": {
        "id": "f_HgyyO9I5AM"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for i,value in enumerate(encodings):\n",
        "  for ind,unique in enumerate(labels):\n",
        "    if value==unique:\n",
        "      encodings[i] = ind"
      ],
      "metadata": {
        "id": "-rLjCOWcJINa"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encodings = np.array(encodings)"
      ],
      "metadata": {
        "id": "99rsc72XJIU6"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "y = encodings"
      ],
      "metadata": {
        "id": "YbYXCJodJIfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking types"
      ],
      "metadata": {
        "id": "J8cABT2dFkCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all the arrays to same data type\n",
        "\n",
        "X = np.array([val.astype(np.float64) for val in X])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5dYV-1CFjOq",
        "outputId": "14dc2fc0-79bc-4721-95df-3b484993e41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for null values in the array"
      ],
      "metadata": {
        "id": "Nhc25PhbM6k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X).isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEArqpIDM5R-",
        "outputId": "7de70cbb-8d2d-49fa-8b9d-132a1a2e7580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    75\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 227 null values"
      ],
      "metadata": {
        "id": "amLM-wY8NA6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us combine the dataset and get rid of any null values that may have occured after preprocessing\n",
        "\n",
        "vector_data = pd.concat([pd.DataFrame(X),pd.DataFrame(y)], axis=1)"
      ],
      "metadata": {
        "id": "67qxnjvcM5XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "yG3k9XQPNHsp",
        "outputId": "65fb6a57-ee12-4e3b-cd55-486a400e3f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0  0\n",
              "0  [0.5185931921005249, -0.2783828675746918, -0.1...  0\n",
              "1  [0.3185077905654907, -0.1711403876543045, -0.1...  1\n",
              "2  [0.39131638407707214, -0.21000224351882935, -0...  2\n",
              "3  [0.24084103107452393, -0.13017770648002625, -0...  1\n",
              "4  [0.23725177347660065, -0.12525340914726257, -0...  3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f5bd467-9570-4c38-9191-ee418e308e19\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.5185931921005249, -0.2783828675746918, -0.1...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.3185077905654907, -0.1711403876543045, -0.1...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.39131638407707214, -0.21000224351882935, -0...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.24084103107452393, -0.13017770648002625, -0...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.23725177347660065, -0.12525340914726257, -0...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f5bd467-9570-4c38-9191-ee418e308e19')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f5bd467-9570-4c38-9191-ee418e308e19 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f5bd467-9570-4c38-9191-ee418e308e19');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data.isna().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqjqVQLPNHvB",
        "outputId": "c685d4a0-95be-4e46-eb6f-01dcd426a85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     True\n",
              "0    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping all the null values"
      ],
      "metadata": {
        "id": "uibrYSgwNZqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "IzaBHu7SNHy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoPqfsDLNH1D",
        "outputId": "89147518-f3e6-4c0a-c6cc-f70d3a2a0474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8272, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adcOf2r7Ipg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([feat for feat in vector_data.iloc[:,0]])\n",
        "y = np.array([label for label in vector_data.iloc[:,1]])"
      ],
      "metadata": {
        "id": "TTVYNjgtNH5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X,y, train_size = 0.93, random_state= 12)"
      ],
      "metadata": {
        "id": "yJpmXLdErQ5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "RFVzraZyx777"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaRfyukTL4Gc",
        "outputId": "2c234576-6de1-4c0d-ae91-f3d45b738e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = gnb.predict(test_X)"
      ],
      "metadata": {
        "id": "7qqIIkV5rRGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "OlVYC7SsOQ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = accuracy_score(test_y, predictions)"
      ],
      "metadata": {
        "id": "tHrQlJJurRLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"And the final score is ...... ..... ...\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv2YvqIHs31U",
        "outputId": "e19131d8-ecff-4120-a36c-b2e03fec82ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And the final score is ...... ..... ... 0.11896551724137931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "id": "XO6JNpqds4EL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6725f0-402a-40b6-c857-e9c0f726df29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7692, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "id": "mw19XNuds4Ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7997a17e-7824-48e6-a0a9-54a1f87432f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7692,)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for v in range(len(vectors[:10])):\n",
        "  print(vectors[v].shape)"
      ],
      "metadata": {
        "id": "URVaZQekacN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddba76c9-e2cc-45f5-cd0e-b959ca83f34a"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 300)\n",
            "(7, 300)\n",
            "(3, 300)\n",
            "(15, 300)\n",
            "(5, 300)\n",
            "(8,)\n",
            "(8, 300)\n",
            "(7, 300)\n",
            "(3, 300)\n",
            "(15, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors[4]"
      ],
      "metadata": {
        "id": "HS7mqYbAacQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84716a4e-77a3-435b-91f6-1899f188f855"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.13769531,  0.03540039,  0.59765625, ..., -0.22167969,\n",
              "        -0.20800781,  0.00340271],\n",
              "       [-0.21875   ,  0.08154297,  0.29492188, ..., -0.5078125 ,\n",
              "         0.02172852, -0.11669922],\n",
              "       [ 0.04785156, -0.26757812,  0.07324219, ..., -0.27148438,\n",
              "        -0.06542969,  0.328125  ],\n",
              "       [-0.04785156,  0.16699219,  0.078125  , ..., -0.29101562,\n",
              "         0.04467773,  0.16113281],\n",
              "       [ 0.08154297, -0.06396484,  0.02526855, ..., -0.14160156,\n",
              "        -0.05395508, -0.07666016]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting to categories"
      ],
      "metadata": {
        "id": "eWwLHCjzzs4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "PwTWvlVCzV4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = to_categorical(train_y,13)"
      ],
      "metadata": {
        "id": "kBvpSoXWzV8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Covertiing test_y to binary \n",
        "test_y = to_categorical(test_y,13)"
      ],
      "metadata": {
        "id": "UBStd_0dEIHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY"
      ],
      "metadata": {
        "id": "8RnOqTwSb53z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "Wq8Nf_TAb-EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "mujCAxBnb-HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "a_HQeLUttL-9",
        "outputId": "6ec28d68-53c1-4bc5-c9c1-ed231f0cbf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Content sentiment\n",
              "0  supersense ooooo explanation thank god u would...      love\n",
              "1                            last day senior bye bff   sadness\n",
              "2  noiselesssound heard regina girl song le deux ...   sadness\n",
              "3  someone said wolverine feel like watching x me...       fun\n",
              "4  shanselman still class loader even custom asse...     empty"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62a6909e-7ba0-47eb-909d-74bbe446759f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>supersense ooooo explanation thank god u would...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>last day senior bye bff</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>noiselesssound heard regina girl song le deux ...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>someone said wolverine feel like watching x me...</td>\n",
              "      <td>fun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shanselman still class loader even custom asse...</td>\n",
              "      <td>empty</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62a6909e-7ba0-47eb-909d-74bbe446759f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-62a6909e-7ba0-47eb-909d-74bbe446759f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-62a6909e-7ba0-47eb-909d-74bbe446759f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = p_data['Content']\n",
        "y= p_data[\"sentiment\"]"
      ],
      "metadata": {
        "id": "EpMKgtXhta85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "corresponding_num = []\n",
        "\n",
        "encodings = [val for val in y]\n",
        "\n",
        "for ind,lab in enumerate(y.unique()):\n",
        "  labels.append(lab)\n",
        "  corresponding_num.append(ind)\n",
        "\n",
        "for i,value in enumerate(encodings):\n",
        "  for ind,unique in enumerate(labels):\n",
        "    if value==unique:\n",
        "      encodings[i] = ind\n",
        "\n",
        "encodings = np.array(encodings)\n",
        "\n",
        "y = encodings"
      ],
      "metadata": {
        "id": "xkpvR-4tvVQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y , test_size=0.11, random_state=10)"
      ],
      "metadata": {
        "id": "uFYVEpl8tT7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sent = [sent for sent in train_X]\n",
        "test_sent = [sent for sent in test_X]"
      ],
      "metadata": {
        "id": "bTerr2FltIgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = vectorizer.fit_transform(train_sent)\n",
        "test_X = vectorizer.transform(test_sent)"
      ],
      "metadata": {
        "id": "qS5jnMdYb-Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "cYojcjOIlMmb",
        "outputId": "fa773d7c-bee1-41ba-ac9c-ed1111fc964a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'supersense ooooo explanation thank god u would forever wondering love good night mare tho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Covertiing test_y to binary \n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "train_y = to_categorical(train_y,13)"
      ],
      "metadata": {
        "id": "e0BYa0BSb-T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y = to_categorical(test_y,13)"
      ],
      "metadata": {
        "id": "PqwR1KIZu0qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqxLSuNnvhue",
        "outputId": "f6f0a82c-e8b5-4389-f6ea-c8564c393e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7430, 13539)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_uu-D8Kdh8"
      },
      "source": [
        "## LSTM RNN MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9_UQpFN-iD"
      },
      "source": [
        "Implementing Bi-directional Long short term Memory recurrent neural network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "5XctiZrBJqNu"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "\n",
        "import tensorflow \n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.layers import Dense, Flatten, Input, LSTM, Bidirectional, Embedding, Dropout, CuDNNLSTM, GRU\n",
        "from keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FX_cBlE8iHN",
        "outputId": "1c1151c8-7981-48cc-dc19-e7c77ad43f7a"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7096,)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fluctuations are normal within certain limits and depend on the fact that you use a heuristic method but in your case they are excessive. Despite all the performance takes a definite direction and therefore the system works. From the graphs you have posted, the problem depends on your data so it's a difficult training. If you have already tried to change the learning rate try to change training algorithm. You would agree to test your data: first compute the Bayes error rate using a KNN (use the trick regression in case you need), in this way you can check whether the input data contain all the information you need. Then try the LSTM without the validation or dropout to verify that it has the ability to achieve the result for you necessary. If the training algorithm is not suitable you should have the same problems even without the validation or dropout. Just at the end adjust the training and the validation size to get the best result in the test set. Statistical learning theory is not a topic that can be talked about at one time, we must proceed step by step.\n"
      ],
      "metadata": {
        "id": "ANsSvX-5HvnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "source :https://stats.stackexchange.com/questions/345990/why-does-the-loss-accuracy-fluctuate-during-the-training-keras-lstm"
      ],
      "metadata": {
        "id": "cdaMQD09qzdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(300,1))\n",
        "lstm = GRU(3, return_sequences=True)(input)\n",
        "dropout = Dropout(0.2)(lstm)\n",
        "lstm2 = GRU(3, return_sequences=True)(dropout)\n",
        "dropout2 = Dropout(0.2)(lstm2)\n",
        "flatten= Flatten()(dropout2)\n",
        "prediction = Dense(13, activation=\"softmax\")(flatten)"
      ],
      "metadata": {
        "id": "YJvqoSDsy9-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "model = Model(inputs = input, outputs = prediction)"
      ],
      "metadata": {
        "id": "zS1hnHUt6ZNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSAZhDi-6e3b",
        "outputId": "c6cc337a-a230-49e2-b763-0f3538ddda8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 100, 1)]          0         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 100, 3)            54        \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 100, 3)            0         \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 100, 3)            72        \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 100, 3)            0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 300)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 13)                3913      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,039\n",
            "Trainable params: 4,039\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the learning rate for the optimizer. \n",
        "\n",
        "adam_optimizer = keras.optimizers.Adam(learning_rate=1e-3, decay=1e-6)\n",
        "\n",
        "# Compiling the model\n",
        "\n",
        "model.compile(optimizer=adam_optimizer, loss=\"categorical_crossentropy\", metrics=\"accuracy\")"
      ],
      "metadata": {
        "id": "OoEALr471j2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras callbacks"
      ],
      "metadata": {
        "id": "uaxBoFlHAySp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "YAjvM2gpA6KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patience = EarlyStopping(patience=200)\n",
        "\n",
        "save_best = ModelCheckpoint(\"lstm_model.h5\", save_best_only=True)"
      ],
      "metadata": {
        "id": "RwBTC950A6rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_ZENR97Dzs7",
        "outputId": "fc19b0e5-cf49-4526-afc9-42229cd34226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7692, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwqYrrw2D2GA",
        "outputId": "2ed9226a-c832-43bf-bb67-986263b5dc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7692, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l51f7vXD2JZ",
        "outputId": "db4a4fc0-719f-4c6e-f66f-dc2a31b6bea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(580, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PogsbxKnD2M4",
        "outputId": "67193f86-6b19-49df-da76-e8e473468dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(580, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzmax9KBvyTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X, train_y, validation_data=(test_X,test_y),epochs=250,callbacks=[save_best, patience])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qwu7JZws1tJR",
        "outputId": "0eaa44de-1559-43f9-a8ca-ace63056a0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "241/241 [==============================] - 4s 15ms/step - loss: 2.3962 - accuracy: 0.0963 - val_loss: 2.3958 - val_accuracy: 0.1052\n",
            "Epoch 2/250\n",
            "241/241 [==============================] - 2s 9ms/step - loss: 2.3964 - accuracy: 0.0982 - val_loss: 2.3944 - val_accuracy: 0.1052\n",
            "Epoch 3/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3952 - accuracy: 0.1022 - val_loss: 2.3913 - val_accuracy: 0.0948\n",
            "Epoch 4/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3956 - accuracy: 0.1027 - val_loss: 2.3900 - val_accuracy: 0.0983\n",
            "Epoch 5/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3962 - accuracy: 0.0996 - val_loss: 2.3938 - val_accuracy: 0.1086\n",
            "Epoch 6/250\n",
            "241/241 [==============================] - 2s 9ms/step - loss: 2.3951 - accuracy: 0.1021 - val_loss: 2.3940 - val_accuracy: 0.0914\n",
            "Epoch 7/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3948 - accuracy: 0.0995 - val_loss: 2.3936 - val_accuracy: 0.1034\n",
            "Epoch 8/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3947 - accuracy: 0.1021 - val_loss: 2.3976 - val_accuracy: 0.0983\n",
            "Epoch 9/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3957 - accuracy: 0.0946 - val_loss: 2.3922 - val_accuracy: 0.0966\n",
            "Epoch 10/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3944 - accuracy: 0.0970 - val_loss: 2.3888 - val_accuracy: 0.1086\n",
            "Epoch 11/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3939 - accuracy: 0.1015 - val_loss: 2.3938 - val_accuracy: 0.1034\n",
            "Epoch 12/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3948 - accuracy: 0.0974 - val_loss: 2.3954 - val_accuracy: 0.1000\n",
            "Epoch 13/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3944 - accuracy: 0.1045 - val_loss: 2.3907 - val_accuracy: 0.1207\n",
            "Epoch 14/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3941 - accuracy: 0.1008 - val_loss: 2.3918 - val_accuracy: 0.1052\n",
            "Epoch 15/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3942 - accuracy: 0.1053 - val_loss: 2.3912 - val_accuracy: 0.1000\n",
            "Epoch 16/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3943 - accuracy: 0.1021 - val_loss: 2.3946 - val_accuracy: 0.0948\n",
            "Epoch 17/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3951 - accuracy: 0.0987 - val_loss: 2.3938 - val_accuracy: 0.1069\n",
            "Epoch 18/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3942 - accuracy: 0.1009 - val_loss: 2.3899 - val_accuracy: 0.0966\n",
            "Epoch 19/250\n",
            "241/241 [==============================] - 3s 13ms/step - loss: 2.3939 - accuracy: 0.1019 - val_loss: 2.3919 - val_accuracy: 0.1086\n",
            "Epoch 20/250\n",
            "241/241 [==============================] - 4s 15ms/step - loss: 2.3941 - accuracy: 0.0995 - val_loss: 2.3936 - val_accuracy: 0.1069\n",
            "Epoch 21/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3941 - accuracy: 0.0983 - val_loss: 2.3912 - val_accuracy: 0.1103\n",
            "Epoch 22/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3935 - accuracy: 0.1035 - val_loss: 2.3941 - val_accuracy: 0.0966\n",
            "Epoch 23/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3933 - accuracy: 0.1015 - val_loss: 2.3912 - val_accuracy: 0.1155\n",
            "Epoch 24/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3936 - accuracy: 0.1057 - val_loss: 2.3905 - val_accuracy: 0.0948\n",
            "Epoch 25/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3941 - accuracy: 0.1039 - val_loss: 2.3918 - val_accuracy: 0.1069\n",
            "Epoch 26/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3924 - accuracy: 0.0992 - val_loss: 2.3923 - val_accuracy: 0.1000\n",
            "Epoch 27/250\n",
            "241/241 [==============================] - 3s 10ms/step - loss: 2.3936 - accuracy: 0.0983 - val_loss: 2.3923 - val_accuracy: 0.1000\n",
            "Epoch 28/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3935 - accuracy: 0.0987 - val_loss: 2.3913 - val_accuracy: 0.1034\n",
            "Epoch 29/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3930 - accuracy: 0.1054 - val_loss: 2.3924 - val_accuracy: 0.1000\n",
            "Epoch 30/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3938 - accuracy: 0.1006 - val_loss: 2.3915 - val_accuracy: 0.0966\n",
            "Epoch 31/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3929 - accuracy: 0.1039 - val_loss: 2.3924 - val_accuracy: 0.0931\n",
            "Epoch 32/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3935 - accuracy: 0.0988 - val_loss: 2.3914 - val_accuracy: 0.0914\n",
            "Epoch 33/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3934 - accuracy: 0.1002 - val_loss: 2.3921 - val_accuracy: 0.1069\n",
            "Epoch 34/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3934 - accuracy: 0.1031 - val_loss: 2.3914 - val_accuracy: 0.1069\n",
            "Epoch 35/250\n",
            "241/241 [==============================] - 3s 12ms/step - loss: 2.3941 - accuracy: 0.1014 - val_loss: 2.3912 - val_accuracy: 0.1052\n",
            "Epoch 36/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3927 - accuracy: 0.1041 - val_loss: 2.3889 - val_accuracy: 0.1069\n",
            "Epoch 37/250\n",
            "241/241 [==============================] - 2s 10ms/step - loss: 2.3931 - accuracy: 0.1011 - val_loss: 2.3911 - val_accuracy: 0.1086\n",
            "Epoch 38/250\n",
            "241/241 [==============================] - 3s 13ms/step - loss: 2.3938 - accuracy: 0.1005 - val_loss: 2.3912 - val_accuracy: 0.0948\n",
            "Epoch 39/250\n",
            "206/241 [========================>.....] - ETA: 0s - loss: 2.3940 - accuracy: 0.1012"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-878b00a94559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m       \u001b[0mio_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_break\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "FRuvGjLNCO8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426e032f-36c6-4c4b-ab93-769359a0640a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['msstacy well thanks thinking ever get scratch one well right',\n",
              " 'totally forgot thats friday till read alyssanoelled tweet ha feel dumb take breath',\n",
              " 'take antibacterial school clean hand cant go loo',\n",
              " 'ever stepped slug accident hate bug',\n",
              " 'treeincally got adjusting mate online shop zzzzzzzzzzzzzzz boring',\n",
              " 'allyycase trying sleep working',\n",
              " 'jackfm sulking free day',\n",
              " 'much amazing day pervert ruined',\n",
              " 'maariiaan hahahah yeah right plus twitteraddict find well almost day likely',\n",
              " 'darenyeow oh wow really good think im going use one lol',\n",
              " 'dear allergy hate please go away love always victim',\n",
              " 'skoosie probably hate happens',\n",
              " 'even arnold save park http bit ly wsm j providing job',\n",
              " 'carlkr please wear glass next video look amazing',\n",
              " 'bummed even one testimonial flickr',\n",
              " 'dammit slept even le weekend enjoy azeroth bbl',\n",
              " 'abeen good question nepal pm declaring resign actually resigning two different thing',\n",
              " 'hope tonight okay night',\n",
              " 'emmagriffiths community see one moderator niandra told',\n",
              " 'bvictor nerve think im sure',\n",
              " 'stargirlreads cantttt grandparent',\n",
              " 'mileycyrus http twitpic com hac dont like justin mean somthing miley respect love miley',\n",
              " 'r damn exam ever gone b done wanna b sun',\n",
              " 'karmina happened dm need txt phone defunk',\n",
              " 'editing picture',\n",
              " 'teamcyrus welcome',\n",
              " 'bbolin going anti social',\n",
              " 'dritan dam want android stupid rogers',\n",
              " 'quot ride one catch one summer til pop open one quot',\n",
              " 'riotgirl sound fun',\n",
              " 'day dayofservice completed aching clearing tree around beautiful lake splitting headache tho',\n",
              " 'final thought day deodorant really make lung bleed inhaled constantly certain female paranoid bo',\n",
              " 'dsml dont depress mother law staying weekend someone else wait kid',\n",
              " 'petethevet thanks',\n",
              " 'cgervcracker nice',\n",
              " 'islandidea knew meant',\n",
              " 'greegreece little thing link u england',\n",
              " 'good girl work done',\n",
              " 'really want camera',\n",
              " 'danabrunetti give direction hjaia covington',\n",
              " 'want watch naruto shippuuden missed episode',\n",
              " 'love already got plan going queen mary',\n",
              " 'stupid playstation stupid controller work play kingdom heart',\n",
              " 'stupid ipod taking forever load',\n",
              " 'kid love dark hair say dye much never know color going yea hahahahaha',\n",
              " 'sending twitter club twittered u',\n",
              " 'brotallybeh dude new star trek burger king commercial bud light burger king proud u man',\n",
              " 'carriestephens mine matter phone',\n",
              " 'happy birthday whore',\n",
              " 'gettin home frm hangin herman',\n",
              " 'saw new star trek movie yesterday twas good quiet day today gym lunch',\n",
              " 'studio ghibli year ponyo http bit ly whar always seems perfect waiting till august',\n",
              " 'imaclassic hey',\n",
              " 'jodiekearns weekend expecting way hot',\n",
              " 'cool lil night berrie eat pizza waitin noel president',\n",
              " 'nahtuhlee sn yeah sad day',\n",
              " 'back online',\n",
              " 'gahh freaking lip ring going death never cooraperates im jack haha',\n",
              " 'bbq gone well weather stayed good right bonus bed training morning work',\n",
              " 'thisisbenwood wanna go see edinburgh happen though haha yir lucky funnnn xxx',\n",
              " 'ah lex ah hey alexa heard right looking forward causing mischief friday',\n",
              " 'kmf going smoothly far free stuff always plus',\n",
              " 'yasmine lol take back lost receipt',\n",
              " 'shopping cleaning bmfing webcam chatting nephew nothing spesh good bank holiday monday nonetheless',\n",
              " 'jimmers u dirrrrrty oh pick hawk figured needed longshot better bet car',\n",
              " 'get boy doc persistent cough good http plurk com p x',\n",
              " 'oh yeah camera clipping problem void completely fixed yay fiddling',\n",
              " 'phone stubborn think gene something',\n",
              " 'well done visteon belfast year salary paid show happens stand',\n",
              " 'anyone eat hot pocket completely satisfied',\n",
              " 'neestaples thx ff followfriday',\n",
              " 'talk lates',\n",
              " 'gingeebee sound horrid sending one buddythepug special pughugs look lot cuddle treat',\n",
              " 'going die tomorrow night emilazy',\n",
              " 'jackalltimelow wish could go melbourne show parent let cry',\n",
              " 'thundered reallyreally loud work little boy started cry wanted cry hate storm wah',\n",
              " 'illegal date',\n",
              " 'exactly hate driving thru dandy sittin light next car asian r callin amp held knife scum',\n",
              " 'seansmithsucks would looking forward seeing tonight going',\n",
              " 'boilerfan damn favorite part lol ok keep stock alot',\n",
              " 'laptop idiot back normal',\n",
              " 'ooooh kay time stop twitterin get going',\n",
              " 'yay research paper finally half page lt going bus bad',\n",
              " 'id happy thats friday didnt work tomorrow blah',\n",
              " 'god hate scary movie fun wimpy',\n",
              " 'mcflyxxdannyxx lol ly',\n",
              " 'female male year old lab looking new home family loosing home must stay together contact interested',\n",
              " 'hate living fl missing ga like crazy like say dont know got till gone ga mind',\n",
              " 'tonight party w girl minus vita',\n",
              " 'stvwrnr believe',\n",
              " 'finished studying abnormal psychology eek still two day fine',\n",
              " 'zoeatthedisco lol hell yes keen going skiing treble cone sometime winter',\n",
              " 'stevyncolgan confused',\n",
              " 'back boring cricket match canterbury supposed big hitting one shame',\n",
              " 'getting rather annoyed notebook know old got wrinkle never slow',\n",
              " 'want make music profile money know write music know play instrument studio',\n",
              " 'getting used shoulder immobilizer thing even getting used sleeping recliner',\n",
              " 'hyperopia afternoon u got new pic nice spec',\n",
              " 'eugenechua tasmania devil mad cold bad start',\n",
              " 'aww thats wikid need book myne sooon miss lovely im ned brekkie speak soon dudde xxx',\n",
              " 'jennifalconer wow never seen bloody awesome think bcoz george last year however could bad x',\n",
              " 'looking photo inspiration everything want required space',\n",
              " 'okay people fones charged',\n",
              " 'headed hospital take pain anymore',\n",
              " 'asombroso bahahaha love gabriel absolutely fucking hilarious',\n",
              " 'halfwelshdragon cake sure look good took one ok ummm good tks',\n",
              " 'get hang kaitlyn',\n",
              " 'yay power phone charged hot',\n",
              " 'say raining http plurk com p x ydn',\n",
              " 'ever restaurant item menu lunch decision hard',\n",
              " 'hooked life sheesh crochet bah humbug sleep um suppose someday',\n",
              " 'chillin neighbor',\n",
              " 'thebeatles time reissue thebeatles album vinyl yes would fab fab fab',\n",
              " 'zactak know loved',\n",
              " 'hannahassan go mid valley la boob haha never mind la wanna go anyways',\n",
              " 'listening new green day album finger crossed',\n",
              " 'haleyfaye',\n",
              " 'queenqh indeed',\n",
              " 'see lovely mum maybe star trek',\n",
              " 'happy mother day mommy',\n",
              " 'ie nearing according stats ff adoption rate amazes',\n",
              " 'got fist fight old biker guy dare even ask yes kicked as',\n",
              " 'looking window depressing beautiful',\n",
              " 'im quite upset really look must thought something whatt thats unfair',\n",
              " 'asking everyone please join praying mom got home er go see heart doctor',\n",
              " 'shower back bed sick still',\n",
              " 'lolzyluvsjb yeaa sad watched live performance peru etc epic wish come n lol xo',\n",
              " 'happy robbie drank good bit long wake sick',\n",
              " 'better get sleep dodger game mother day maro ewl wake',\n",
              " 'mandyyjirouxx today mtv allow u vote',\n",
              " 'flat sparkling clean team work way pick food',\n",
              " 'wish could commencement yr wonder really going protest',\n",
              " 'ralphp eagerly anticipating twitpics',\n",
              " 'make nice cup tea pop benadryl long day stay positive twitterbugs goodnight',\n",
              " 'even bother getting new fone fucking break anyway fones dont float especially thrown pool',\n",
              " 'yep think might made decision go head first may witnessing historical moment real talk',\n",
              " 'definitely need work business trip much free food alcohol sure gained pound',\n",
              " 'see result whyareyoustillhere',\n",
              " 'andyacb yes putting inside anything else',\n",
              " 'clear intellectually severing relationship best yet heart seems b complete disagreement hurt much',\n",
              " 'hmm feel like writing monthly report tonight probably get started though',\n",
              " 'mauvedeity umm ya mean ship',\n",
              " 'headed parking facility turn permit renewal across breakroom mean check snailmail month',\n",
              " 'killing people realy bored although sound like fun',\n",
              " 'work go',\n",
              " 'eat reallyyy good ice cream',\n",
              " 'need buy bluetooth device machine',\n",
              " 'apdiggles mother favorite hard telling',\n",
              " 'awake man need go sleep gots wake early church tomorow',\n",
              " 'hey dont feel good cuz got car accedent yesterday',\n",
              " 'jonathanrknight sorry hear ur flight got cancelled blow',\n",
              " 'wow install twitter fox tired keep refresh browser',\n",
              " 'dhughesy congrats hughesy holly safe arrival rafferty david hughes hope well xoxo',\n",
              " 'hopefully going back work tomorrow tired flu beside love working child fun play',\n",
              " 'well sucked except company go kelly',\n",
              " 'slept woke iced coffee lazed amp went late lunch bf sweet little laid back saturday',\n",
              " 'like living dream land always get want god thank everything grateful lovely life',\n",
              " 'lady lynn deanna raquel ahh man ok end doin sumthin else let kno meet see wht yal gone',\n",
              " 'chillaxing really yeah man hehe',\n",
              " 'chellemariee class fuckin school time work wed jade come ima get antioch amtrak',\n",
              " 'fuzzyorange thought people went hounslow gone wrong way know anything',\n",
              " 'reboot laptop factory setting losing everything process',\n",
              " 'ravikapoor thanks really fun love romance comedy though one could used comedy imo',\n",
              " 'got super cold',\n",
              " 'tomfelton seen girl http bit ly lvi feel bad',\n",
              " 'day dreaming fuzzball snl http fuzz ball com twitter',\n",
              " 'robinlefeber ojee suck',\n",
              " 'yeah im gonna take ur picture ipod baby',\n",
              " 'wanna happy',\n",
              " 'drea bbcrew dude im sleepy went bed soon got home whhaacck',\n",
              " 'gotta get used pocketwit damn twikini',\n",
              " 'adeelahmad rofl u hahaha lemme tell salman munir p p',\n",
              " 'heyennovy haha balcony seat great giant dragon thing glowing red eye move wing',\n",
              " 'dreamt monster last night like little girl',\n",
              " 'got smoking cig drinking coffee need start working final project univ',\n",
              " 'aim getting ready go bed running k tommorow mother day wish luck',\n",
              " 'cough drop taste gross blaaaqhhh',\n",
              " 'eating hawain pizza breakfast bit cold',\n",
              " 'hour work sunday boo find time two hour lunchbreak though yeah',\n",
              " 'makinitrite',\n",
              " 'miguel n get',\n",
              " 'going bed hung w aaron robin took aaron sunnys',\n",
              " 'starbucks love eff school work later',\n",
              " 'thanking god elton allowing see new tw trailer since work blocked youtube lj',\n",
              " 'etdragon playing good big baby',\n",
              " 'xhayleeey dont really mean anything anymore l good old top pop every sat friday something',\n",
              " 'hate fucking cold stop sneezing claratyne work hr',\n",
              " 'smell cake see cake oven',\n",
              " 'almcheese alms ur tweet r delicious haha since foodddd anyway gatau nih aku mau pindah dr binus',\n",
              " 'omg didnt even know friend knew ijustine lol small world',\n",
              " 'much day foot place day',\n",
              " 'chrisdjmoyles sorry spoil time',\n",
              " 'morning everyone another dat school',\n",
              " 'ninjapixie sorry loss know feel lucky cat',\n",
              " 'listening nine yr old murdering hannah montanna disney sing bring headache tablet',\n",
              " 'dammit forgot go canvas shopping today',\n",
              " 'shake sadly robot song library',\n",
              " 'happy mother day mother beautifully blessed day',\n",
              " 'depressed ty much',\n",
              " 'madonnacalling fav cd',\n",
              " 'plbrickner mind kent best friend going im going miss',\n",
              " 'patriciaco youtube really awesome quality actually',\n",
              " 'thecubs cubby even else new matter beat dem bum chicago baseball',\n",
              " 'johnwyattedgar dont really trust judgement vouches brandy xo jk think becoming friend',\n",
              " 'tivon lmao saw sound hella good hair appt',\n",
              " 'aimeenewell oh god bless',\n",
              " 'sroxy good morning break digit yet',\n",
              " 'dear rain suck gotta change plan tonight',\n",
              " 'goodlemax send two coldplay song clock viva la vida',\n",
              " 'mirandabuzzfans dont worry hate nobody comment pic',\n",
              " 'jamespenycate massive beer guardian huuuge told already trust',\n",
              " 'worst day ever graduation',\n",
              " 'need cheering',\n",
              " 'join biggest bestest group facebook http bit ly cdrbt',\n",
              " 'feeling much better history research',\n",
              " 'hillaryhatt never liked boy jerms',\n",
              " 'zsafwan much welcome',\n",
              " 'jaybranch remember kid grenade go soon tidied house',\n",
              " 'cinnamoncloud well ether buy dvd wailt till july see secret episode episode',\n",
              " 'stevetilley lonely battle friend fought valiantly',\n",
              " 'great day training fun',\n",
              " 'sweat sweat deal thought sweet start pronoun move onto spelling trentles',\n",
              " 'rochellewiseman http twitpic com jguu wow pretty x',\n",
              " 'hell yeah kellynn got twitter finally',\n",
              " 'anaruba need carseats',\n",
              " 'cmiget always fun clean',\n",
              " 'lrkane lol worry wanna take care ticket eft whatever',\n",
              " 'bored need people rsmv jagex let u say rsmv unless first word u say sentence sad',\n",
              " 'jerryshaw tried changing pic twitter hater im doctor awesome im medical assistant',\n",
              " 'xkirstyxo really good distraction check right',\n",
              " 'greek season two love show',\n",
              " 'always forget much fun kyle',\n",
              " 'let sun catch cry oh cool http tinyurl com cugy c',\n",
              " 'f work want somebody come',\n",
              " 'happy mother day momma momma currently wonderful day',\n",
              " 'candle wax enjoyable',\n",
              " 'somaya reece naw missed video cool looking sexy',\n",
              " 'chelseychapman long time well txt xx',\n",
              " 'authorsaoirse r drive srsly jealous never ever one one rd longer operational',\n",
              " 'passing early river sound amazing cozy bed cozy dog',\n",
              " 'devilworks wow',\n",
              " 'jorge might also want include quot never wear moonwolf quot',\n",
              " 'ce ci im reasonable gotta wake early tomorrow first day holiday great evening',\n",
              " 'tweettwang gotta check bro mine girl',\n",
              " 'pretty good kind depressing though',\n",
              " 'anna worry get bored hang give',\n",
              " 'getting hang twitter',\n",
              " 'sitting mr martin class youtube stupid really want popsicle jealous sara',\n",
              " 'themaguire maybe one day favorite producer list lol',\n",
              " 'bah sun new hammock drinking beer playing guitar singing mosquito chased inside',\n",
              " 'hangover movie gonna hilarious wish could see crew',\n",
              " 'jennyreyn fail fancy pit stop cuppa know lol',\n",
              " 'ugh work suck could sher right',\n",
              " 'glasgirl gaaaaaaasp know final one sad reading book year',\n",
              " 'chaosandharmony mo beck guessing met tga wbw spoke last yr business holiday',\n",
              " 'going look refrigerator taste snack eat',\n",
              " 'new comic posted introducing quot joe mini strip quot http tinyurl com oasxx via mbillingsley cute mother day strip',\n",
              " 'found quot friend quot isnt actually hey shit happens',\n",
              " 'drronvondoom derrickjdavis might swine flu haha cause got flu',\n",
              " 'kellychiello know weather clearing suppose nice sat sun',\n",
              " 'lesliein u see twitted real xmsirius let know contact info let transmit',\n",
              " 'splinteredmind anticipating next ten day',\n",
              " 'cough cough hack hack',\n",
              " 'supposed m twitterology',\n",
              " 'aw torn ace heart hunchback',\n",
              " 'heading spin',\n",
              " 'jennjolie yep b automatic u fall someone person smthng like way suck',\n",
              " 'hey get comlementary cherry flavored lip gloss time cool',\n",
              " 'lovelyaaris fair hungry hell',\n",
              " 'gfalcone hey gio beautiful brazilian love hahahaha please answer xx',\n",
              " 'ubudroi know took u long get around yes takeaway seafood delivery forgot get menu',\n",
              " 'golden girl marathon end lofnotc',\n",
              " 'nianoelle hey nia used pd hot dayton met cincinnati wiz christmas party hope well',\n",
              " 'hiracdelest one always working oh yeah still',\n",
              " 'spoke club fell love tom arnold rosie ex definitely different lucky moi',\n",
              " 'ahh forgot test today',\n",
              " 'reeljpmorgan tellin u pretty mean kitchen lolol yummy',\n",
              " 'http twitpic com wah haha right',\n",
              " 'prmack community news sometimes accept guest post moment',\n",
              " 'tackstorelady coming today get hat credit',\n",
              " 'trainright http twitpic com vgzy moving great especially lot help',\n",
              " 'buddythepuggy poor buddy teased mini puggy think need pughug',\n",
              " 'princessmargo saw none baddie best',\n",
              " 'oficially back work system running smoothly',\n",
              " 'ferarro blog set private',\n",
              " 'bored homework done master program friday nite wit nutin',\n",
              " 'came back bishopstorford went aunt wedding party way fun got see cousin year',\n",
              " 'mess ilove',\n",
              " 'melziec saw show listing saw wednesday',\n",
              " 'lon colossus lol cheer mate yeah went well thanks',\n",
              " 'love baby hurt hurt want watch night ruxbury right',\n",
              " 'hey yeg anyone goin edmonton energy game wanna live update please dont post live score',\n",
              " 'shot suuuuck im done vacines',\n",
              " 'tremblah wishhh',\n",
              " 'kat xxx',\n",
              " 'binstruct suffers update mib package developer package holiday',\n",
              " 'inkophile weekend like almost always end quot one week quot',\n",
              " 'finished little booty duty work hungry',\n",
              " 'spent hour morning going yearbook see senior started harleton speech boring',\n",
              " 'jdennes thank first follower',\n",
              " 'hate see pregnant woman smokinggg sooo irresponsible n selfish saaad ultimately baby one suffers',\n",
              " 'thatdude someone hate',\n",
              " 'ocean sound cd',\n",
              " 'tweetvisor happened real time feed friend feed like twitter website',\n",
              " 'horrible appraising peer work american thinking compose poem maybe one problem',\n",
              " 'hd full http plurk com p x eb',\n",
              " 'hey roll everybody twitter ville sleeping grrr never win http myloc j',\n",
              " 'heatherjoy figure work get hope well',\n",
              " 'planned better surprise party hubby',\n",
              " 'karaoke small town bar wonderful time',\n",
              " 'candicunningham oh dear u serious one prevent bite itchy distracting editing sigh',\n",
              " 'turnontheradiox sure ill follow hun ohh thank subscriber youtube',\n",
              " 'im depressed pretty day everyone either something bleh',\n",
              " 'uploading video youtube give link get loaded variety daily life amazing fun watch',\n",
              " 'early start today rain mile',\n",
              " 'fromlucy thank dearie followed dome',\n",
              " 'bensabeast whaaaat happen',\n",
              " 'quot want mouth butthole b tch quot mm jeffree rawks',\n",
              " 'gotta go bye twitterland good night try sleep recover easily still sick bye',\n",
              " 'catticho diego know lo ame jack',\n",
              " 'weekend sunny got another puncture',\n",
              " 'im angry right today im doind nothing classmate yes think stay friend mr computer hope',\n",
              " 'coybh yep rock station okc check profile real quick',\n",
              " 'duckout whole earth festival',\n",
              " 'davidsadof sure tweet business simple thing difficult',\n",
              " 'millionfagmarch ah misunderstood message really dislike f word url good luck march',\n",
              " 'got back lunch feeling like nap order work press',\n",
              " 'rehearsal gonna miss game please keep updated go nugget',\n",
              " 'love black eyed pea many memory watching hulk',\n",
              " 'msz rockstar umm bout em aint get see da game see dey survived anotha game da series due notin lebron im guessin',\n",
              " 'good morning want breakfast',\n",
              " 'kenichan fully grabbyhands bad candy thanks lt',\n",
              " 'tdes thanks asking sarah glad tomorrow th holiday korea rest right rain',\n",
              " 'trying buy new bible dog chew',\n",
              " 'okay yesterday good went food shopping cooked chicken taco bakes cooky back work',\n",
              " 'xaviermathews sexy tube sticking stomach thanks',\n",
              " 'sitting almost empty dorm waiting jordan come take last thing say good bye graduate tomorrow',\n",
              " 'watched gossip girl dan funny haha',\n",
              " 'dizzybunny haha rite im freeeeeeee britneys spear listenin time',\n",
              " 'fell stair dancing sword fell bum hurt',\n",
              " 'decided wolf future star trek logo game would much cooler chewy star war',\n",
              " 'new french girl twitter speak english bad',\n",
              " 'kicesie elope wait see picture dress breath taking',\n",
              " 'yermilla really bcoz good math amp r learning sumthing need thats like math amp teacher uh',\n",
              " 'watching snl guess justin timberlake omg funny hahahah',\n",
              " 'listening varsity fanclub',\n",
              " 'simonferrari meblair thought starbucks licensing store bc take away experience want provide',\n",
              " 'mouth hurt stupid retainer',\n",
              " 'corruptjelly quot icant live cant live quot lmao oh btw oooowwwch foot hurry plaster im going bleed death',\n",
              " 'clairesale poor beta took seattle riding lap plane happy week long outage cold',\n",
              " 'moving back home today pro obnoxiously closer thom con mpls run excursion least year',\n",
              " 'brodyjenner u watch hill london u realise tourture week week late watch itonlinelol',\n",
              " 'dougiemcfly cold pal',\n",
              " 'thinking wow survived freshman year mission accomplished sophomore year',\n",
              " 'must hve graduation practice im gonna walk',\n",
              " 'going get tattos get paid jack',\n",
              " 'got back six flag wicked fun even tho almost died',\n",
              " 'mismile u gonna host saturday night live waiting like years',\n",
              " 'happy star war day everyone may th',\n",
              " 'downloaded ton stunning beautiful wallpaper www interfacelift com go look',\n",
              " 'urgh feeling like crap today bad headache tired blood sugar high',\n",
              " 'jimmyfallon saw snl diploma congrats must overjoyed finally going bed nite',\n",
              " 'tonight bad night',\n",
              " 'afternoon first rehearsal extremely talented singer songwriter today wish luck',\n",
              " 'potface sorry didnt reply earlier feel better',\n",
              " 'searching place migrate',\n",
              " 'doodlebug sorry darling anything',\n",
              " 'pearlyn place gran bday mother day dinner tonight mom went adult svc today l happy mother day mom',\n",
              " 'quot weather outside weather quot hahah made feel better',\n",
              " 'way birmingham sewing van punk',\n",
              " 'dougiemcfly want walk x',\n",
              " 'footy getting krisnan inu back team also twitter get rid ridiculous character limit',\n",
              " 'midnight amp hear shower calling technically twice one day since morning quot green quot',\n",
              " 'load little job today going playing walking themed music today brickman',\n",
              " 'janine j look chatting star wtg',\n",
              " 'today work tomorrow',\n",
              " 'made bomb blew mouth made best f ing dinner',\n",
              " 'want get hand dirty fubumvc http bit ly j ha document yet complete',\n",
              " 'going grocery store best friend',\n",
              " 'urgh power cut',\n",
              " 'burning cd fucking outa blank disc',\n",
              " 'little hungry nice bowl spicy lentil soup fill gap',\n",
              " 'heycassadee one fake quot quot followed haha wish would instead',\n",
              " 'funny time neball plc score game',\n",
              " 'penelopeoverton george b shaw apparently wrote word day sure practice work come nd nature',\n",
              " 'therealsavannah today fun lt',\n",
              " 'know sprain repetitive injury',\n",
              " 'glad someone slept last night doggy would take picture see laptop',\n",
              " 'let twitter alone',\n",
              " 'mikeyconner hey bought porter cable piece set new drill led light near trigger oh happy',\n",
              " 'sexykellyc hey chocolate chip good want snack snack',\n",
              " 'hype friday amp raining outside rain day rainin time kick mr sun trippin',\n",
              " 'somewhere within temptation returned mysefl keep working hard till june',\n",
              " 'http twitpic com w bwahahahahahaha awesome',\n",
              " 'hmm today suppose could revise science test could eat cooky watch film',\n",
              " 'whyareyoustillhere one tree hill cancelled show game amp everybody hate chris n dont nobody watch u n e',\n",
              " 'trixie trivia working computer',\n",
              " 'gcrecords yes',\n",
              " 'http bit ly aam dog going die somebody save',\n",
              " 'paying per hr internet access may limited twitter via txt',\n",
              " 'mother day spit awesome listening paranoid jonas brother',\n",
              " 'abledragon geek building photoblog theme',\n",
              " 'eatin nacho watchin game gotta say lebron beast anybody feel',\n",
              " 'russellbfan ah getting still fair bit go coming together fairly well aw',\n",
              " 'headache',\n",
              " 'medros try best hope get laid',\n",
              " 'zoelovesarchie zoelovesarchie true cause dirty thought lol well also dude huh xoxo',\n",
              " 'mount dish conquered dish land',\n",
              " 'cleaning room',\n",
              " 'bed sorta today good sara strep thought angelina shared water b told prob get',\n",
              " 'rest peace sheeba missed go cry',\n",
              " 'luketurcotte hulu germany suck',\n",
              " 'back track transcription process still stop lappy overheating though',\n",
              " 'lynnnein bad rum experience college still recovered',\n",
              " 'sjrozas lol leave',\n",
              " 'paintgranny little felted thing look like incestuous product two lovely',\n",
              " 'full thanks food jean brought half watermelon eat freeway crash die',\n",
              " 'emmielovegood nope way home',\n",
              " 'mariav st vaca buuu sigo en el work',\n",
              " 'really degree high way work hagg lake tomorrow going especially bomb sauce',\n",
              " 'tommcfly buonotomato bluejeriberry still sending much tweet philippine tour please',\n",
              " 'sad gmail chat died help natalidelconte',\n",
              " 'thought wuz best friend',\n",
              " 'dahlhalla quot',\n",
              " 'starting nd shift im going miss like hour lakers game',\n",
              " 'mcrisapleasure really wooooo wish could go indonesia lol hopefully next summer',\n",
              " 'blame got good p better good condition lt night',\n",
              " 'sqldba sorry make sure bomb talk ssug next month penance',\n",
              " 'going sleep good night everyone',\n",
              " 'haih cannot sleep lar',\n",
              " 'good mood mama away si talk either boy see weekend planned',\n",
              " 'woke mum singing new gn r cd replacement bought im good daughter',\n",
              " 'mariahcarey mc happy mother day mom love yah',\n",
              " 'good eloise',\n",
              " 'oh lol sorry mind always',\n",
              " 'coupleocachers trying watch vids audio disabled',\n",
              " 'pyroezra know hella wish could mom move across usa',\n",
              " 'think polo th following',\n",
              " 'love pink care today shall tweet hardcore work jamie nice day everyoneeeee xxxxxxloser',\n",
              " 'deeliciouz even without dressing still calorie love flatbread',\n",
              " 'bwexxx hope hoe much fun much without though lol',\n",
              " 'ohshititsdre awww didnt see till edwin left knew wanted forsure tomorrow make u new batch',\n",
              " 'lohang listening music really happy librefm audacious combination profile http http tinyurl com r zj',\n",
              " 'seschloss mine look',\n",
              " 'modeltheany tee beefin u supposed leavin',\n",
              " 'well least bad thought found new website watch movie gotta say bad bad',\n",
              " 'ruthchu wat riverside',\n",
              " 'thebeast oh okay cool love fast furious wait see new one',\n",
              " 'shriya',\n",
              " 'kduggs omg spit drink rip hair straightener',\n",
              " 'airheaduk washing comp great people fab atmosphere st dist nd exp sess',\n",
              " 'clocked possibly last time northview middle school',\n",
              " 'fallenstar get',\n",
              " 'annwhit hiya looking',\n",
              " 'goin bed w smokey',\n",
              " 'la maestrada agree tw cat watching full housse sabrina teenage witch sister sister run',\n",
              " 'rachaely yes haha impaled crossed key love scottish trying irish ewan mcgregor',\n",
              " 'tommcfly come see host hilton mr thomas spent lot money nothing boring',\n",
              " 'billohbill wtf lmao got hit head bloody ball',\n",
              " 'tonight fun love girl tanna',\n",
              " 'chriscuomo seen enough movie quot know quot something terrible happen lol',\n",
              " 'geekwearsprada going trop last year game',\n",
              " 'enjoyed star trek please',\n",
              " 'nathanfillion going uk take',\n",
              " 'today first baseball game go bat',\n",
              " 'maddiewan maddie start working watch world',\n",
              " 'way school last friday high school ever even get see holly gabbie hannah',\n",
              " 'hate midget smfh dream fightin ln boston market turbull kept head bunting',\n",
              " 'steinmoney missed bring back keychain',\n",
              " 'nicotine replacement patch hour far good sleep hour getting bit twitchy',\n",
              " 'catwoman something like',\n",
              " 'noooo favorite coworker got new job marc jacob show fair want go',\n",
              " 'time head back home business done',\n",
              " 'watching midsomer murder totally love show pursued forensic psychology instead brain thingy',\n",
              " 'beautiful day going edit lisa maxwell interview last day bill shame see laptop screen outside',\n",
              " 'stealth shopping got wife b day present mall together notice buy hide car back missed',\n",
              " 'pulled walmart aunt got went fell asleep hour later r lol',\n",
              " 'day summer holiday left going way slow school get',\n",
              " 'free unlimited ringtones http tinyurl com freeringring usa awesome iphone',\n",
              " 'praxilla rock guy world night make pancake next morning',\n",
              " 'make school musical week aaah',\n",
              " 'trader joe quot sushi quot fail',\n",
              " 'omgod soooo tired think energy film today lol',\n",
              " 'hockey fukin good fuck hole xd',\n",
              " 'working late night dell notebook dell quality gone hill warrenty service suck poor people buy mac instead',\n",
              " 'starting look like scifileague website may happen form',\n",
              " 'heatxsink screaaaaaaaaaaaaaaaaaaaaaaaaaaaaam never freu zu fr h',\n",
              " 'friday amp every plan mom laser tagging w friend haha brother sister dad r working bf alone',\n",
              " 'got let go today',\n",
              " 'zombiegrrl thanks feel lot better hurt like hell good luck lousy student writing',\n",
              " 'know want stay cool day use shower gel bath wash peppermint',\n",
              " 'couple od day time need get sleep night night peace',\n",
              " 'saydiemason well',\n",
              " 'yogulicious another sour sally competitor',\n",
              " 'ellyaway know june gonna good im going se even im come',\n",
              " 'kat ever turn part brain talk crap fun come',\n",
              " 'ok outta follower u ive talked lol dont shy dont biteeee',\n",
              " 'thesedreams see shave head love cut glad shave hair pretty',\n",
              " 'usagiii sweet something new show',\n",
              " 'cel xox ur really smart',\n",
              " 'runpaintrunrun wordsnfixtures pimm',\n",
              " 'going scandic food learn leadership course',\n",
              " 'get early feel good day walk work feeling alright guess work today',\n",
              " 'sleep ugh shit damn day tomorrow wanting take sleeping pill know ill get late',\n",
              " 'megpriley sorry never got back going vega weekend asked forever ago fun good luck',\n",
              " 'throw lotion air port terminal',\n",
              " 'im really bored anthony senior board shit im hungry cold',\n",
              " 'going work great day atleast get commin wahooooooo',\n",
              " 'sick right thankful chicken soup bed',\n",
              " 'kelleyviolet suck thundering getting ready shut',\n",
              " 'rasga yep http bit ly yyid go christmas treat',\n",
              " 'well hot heck right',\n",
              " 'themakeupsnob know worth shot though',\n",
              " 'niariley follow',\n",
              " 'never get invited go anywhere',\n",
              " 'googledocs folder instead label like gmail kind like label',\n",
              " 'morning twitties heading college back doc god hungry streching ear today mm payday love',\n",
              " 'wishing watching video freezing next weekend bring edea without dying',\n",
              " 'need cheer',\n",
              " 'wish call blah used easy move wtf happened',\n",
              " 'ikonora great night',\n",
              " 'acsvxdcbgfn soccer shall see young phoebe want dressed though',\n",
              " 'thepioneerwoman bingley practise proposal scene toward end endearing',\n",
              " 'follow dad georgiebouy wont much fabulous say make feel loved',\n",
              " 'melissa nah im pooped moving day im laying relaxing lol',\n",
              " 'havnt gotten month ex seems dating spark anymore still love loser left',\n",
              " 'leinadani haha realised sounded lot like stellllaaa lol anyhoo got ur facebook msg start working soon hopefully',\n",
              " 'tsarnick yeah better sorry believe kiss sas',\n",
              " 'hornyme kiddin shy word use describe',\n",
              " 'marvelvscapcom oops late',\n",
              " 'car warmed sprite taste like sore throat',\n",
              " 'happy mother day',\n",
              " 'ianvisagie',\n",
              " 'ok break back book fun lovely http blip fm z da',\n",
              " 'blokeslib sweet dream babe mwah',\n",
              " 'took time get wall killed lb dummy drag second',\n",
              " 'people eight year junior understand reference',\n",
              " 'mileycyrus u get chance u post video tinkerbell saying peekaboo kinda wana hear say',\n",
              " 'npyskater thank',\n",
              " 'blackout city never good',\n",
              " 'adamrphoto ow shitttt cant come get drunk ihave go photo shoot portsmouth sumfink owwwwwwwwwwwww',\n",
              " 'brenflakes sound like nice weekend lady',\n",
              " 'going newtown soon',\n",
              " 'ogvenoe die lunch date rocio coming plus new shoe ugh',\n",
              " 'http twitpic com wmaw pink green love',\n",
              " 'zuzu get lot sleep last night good thing reading',\n",
              " 'xxmixedmodelxx im schaumburg rite u want ice cream lol type chilly outside lol got ice cream',\n",
              " 'gave homeless lady named ruby ice cream sandwich cigarette g deed day p',\n",
              " 'yoboseiyo heehee know talking',\n",
              " 'windswept tree true kind style le quot old chap quot',\n",
              " 'danget problem wlw blog engine uggh',\n",
              " 'big brother day mean constant live tripe e scrub fall asleep happy',\n",
              " 'feel live without something taken away oh know movin',\n",
              " 'bassisland really brilliant',\n",
              " 'nope thank god air real application compiled code',\n",
              " 'class longggg day',\n",
              " 'chemical romance official sodahead profile http www sodahead com mychemicalromance',\n",
              " 'jalenrose cavs easier route denver',\n",
              " 'esmebella kk follower like minute ago',\n",
              " 'feel like taking day cannot afford looking forward dfb cup final tmrw night though go werder',\n",
              " 'vogonpoetry least denying nerd part',\n",
              " 'see point laugh ugly',\n",
              " 'janehungoz hope presentation went well today tweet yr campus tomorrow around lunch time onwards',\n",
              " 'son bitch arggghhhhhhhhhhh',\n",
              " 'casmonaco still stay want',\n",
              " 'goodnightirene thanks lady',\n",
              " 'finished minish cap great game miss ezlo link',\n",
              " 'mnrmg want travel little',\n",
              " 'andre mitchell never opera think',\n",
              " 'davechapman new baby exciting congrats advance',\n",
              " 'imreallywildin u callin grimmy',\n",
              " 'feel deflated doggy',\n",
              " 'poopiesanchez clearwater',\n",
              " 'blueobsidian always good idea',\n",
              " 'crap break quot work weekend quot rule much overloaded work aargh hate',\n",
              " 'exhausted coming home swim morning tiring remember happy mother day',\n",
              " 'westend enjoying sun',\n",
              " 'man fucked test playing cod day till summer',\n",
              " 'headache want go think worth maybe know',\n",
              " 'http twitpic com wry look camathome rosemary back garden camerabag quot lolo quot mode',\n",
              " 'recovering italian cruise mediterenean',\n",
              " 'arielemoonfire mean b going bed earlier monday',\n",
              " 'papermelody although bit tighter smaaaaller last pair baggy hell get front wedgie',\n",
              " 'near daily exercise starting show result',\n",
              " 'happy star war day',\n",
              " 'e doc getting younger day u guess yup nuh paying ear doc visit ouch',\n",
              " 'clare josa exciting looking forward pic facebook xxx',\n",
              " 'omg great day today went art thingy noe really want zune hd bed part raining come mister sun',\n",
              " 'crystallynn know yayy',\n",
              " 'anyone got ffe account add',\n",
              " 'craving burrito boy large halibut everything jalepenos line hot sauce anybody hook',\n",
              " 'edial theineke',\n",
              " 'erincharde yea afterthought',\n",
              " 'lisa graham yerrrr sameee haha way play edward thinkk mmmm',\n",
              " 'miss read tweet know emo guy dsds',\n",
              " 'making mom mother day card',\n",
              " 'leahchu worst dream ever weird think thought subconscious',\n",
              " 'good start day left money home hot day looking free carparks',\n",
              " 'bioshock fantastic first time played sleep bioshock tomorrow',\n",
              " 'beat heat cool guava juice handi water',\n",
              " 'alibee wave quite u still lounging around morning',\n",
              " 'baddestnla traffic still see sun',\n",
              " 'leischen show always come money least rotten law average',\n",
              " 'raining gng rain dance',\n",
              " 'iyaitssuzanne ohh yeh sexy men okay',\n",
              " 'mother day sunday forget send something special http www youtube com watch v expmtevsfqg',\n",
              " 'ladyfarrahgiano baby fever',\n",
              " 'going program programmed long time',\n",
              " 'arnaudjacobs haha well friend party realized forgot stepmom bday come home call',\n",
              " 'happy mother day everyone',\n",
              " 'reinstalling apps company macbook pro assimilated active directory collective',\n",
              " 'idolart good morning',\n",
              " 'woke delightful nap desribe much success involved nap sat night need rhubarb',\n",
              " 'thylady yay welcome etsy seller world',\n",
              " 'best wknd man levi sara love u guy',\n",
              " 'keep lulion family prayer lil bro jus passed away',\n",
              " 'espn firsttake superman course',\n",
              " 'ill catch last second',\n",
              " 'dane cook sport arena tonight wish ticket',\n",
              " 'tina beanz damn sorry hear tina',\n",
              " 'listening music kostet der fisch xd mathsteacher choose wrong job wrong grammar real fact',\n",
              " 'almost comforting know ldn people working beautiful day get skin cancer want office',\n",
              " 'markhoppus ahaha stuck head thanxx',\n",
              " 'faa thaa night movie night',\n",
              " 'soniaohmae twitter tavern bore much',\n",
              " 'tonight last night jay leno gonna cry like baby',\n",
              " 'selenagomez selena mom congratulation nice day',\n",
              " 'birthday tomorrow jack shit weekend',\n",
              " 'sambennington although wear sunglass see uncomfortable felt poor sam',\n",
              " 'disappointed cadbury chocolate block got smaller',\n",
              " 'guybatty hehe nice',\n",
              " 'dressjunkie know live middle nowhere house spider central',\n",
              " 'smh im sick bug going around almost everyone know sick including',\n",
              " 'josh thanks guy showroom well wanted chk flesh quite cool lamp wife impressed',\n",
              " 'broken wordpress mu powered blog site ok admin screwed',\n",
              " 'know neck jacked forced pay parking bc turn head parallel park free space',\n",
              " 'dexterouslady next best thing bored bored able share people twitter',\n",
              " 'kamathvasanth wait till vacation end u shud cycling lot',\n",
              " 'amytropolis wow sound heavenly quick drive north carolina wait',\n",
              " 'pursebuzz http twitpic com z know always click doughnut picture lol always end wanting one',\n",
              " 'tired work hate closing eek smile today aloha kitchen yum',\n",
              " 'believe puppy like brussels sprout',\n",
              " 'dinner scoop said goodbye senior trip',\n",
              " 'day till half term',\n",
              " 'posting first tweet',\n",
              " 'kiss foot people kick anything want morning everyone hope best day ever',\n",
              " 'okay really going stop making laugh like',\n",
              " 'wolfofmibu need wardrobe intervention',\n",
              " 'say goodnight good morning',\n",
              " 'thx nice quot going alright quot mail yes course leave comment blog cheer napping',\n",
              " 'abiface thanks havin u overrr',\n",
              " 'n nh coi c nh c k ch n l n n b c g n tinh th n v c c kh c nh c n',\n",
              " 'etherjammer think always offtopic sometimes inappropriate germination quot fruit picking quot joke etc',\n",
              " 'great day apart fact bought usb hub soon plugged broke damn tesco',\n",
              " 'lucasblack yeah know thanks much',\n",
              " 'getting used getting cold asthma paying',\n",
              " 'wish boston dmb',\n",
              " 'go work',\n",
              " 'little beetle feeling love search bring zilch bar peep appear funny',\n",
              " 'keithahundred yeah kno tryna change bac lettin meeee',\n",
              " 'cooky milk make feel better thanks babe still recuperating last night http yfrog com izrj',\n",
              " 'enjoying fresh mango bubble tea slush still work',\n",
              " 'so internet billing fee country setup cost usa alone',\n",
              " 'imbase k slice left',\n",
              " 'good friend lt miss much',\n",
              " 'kristi crow happy h haha know would',\n",
              " 'like old day drinkin old spot',\n",
              " 'watching good morning america',\n",
              " 'tommcfly respond ddlovato fan',\n",
              " 'helped save runaway dog want friend though',\n",
              " 'miss nothing except lay beside pool every day summer much better spending day class',\n",
              " 'early morning golf sunny day',\n",
              " 'selling drumset sad day',\n",
              " 'innyvinny thank u tonight',\n",
              " 'algov lmao glad one',\n",
              " 'first aid shift started excitement kinda died wish could often',\n",
              " 'getting little mowing grass evening fun',\n",
              " 'kianlim snap grid realized seen iqbal day',\n",
              " 'sharkara dunno maybe flu feel bitbetter',\n",
              " 'missrachel much',\n",
              " 'happy mother day day chocolate',\n",
              " 'cramp r eew shall hug soft toy tummy n zzz pain away',\n",
              " 'make smile get better believe',\n",
              " 'filos thinking dawn shot carlingford lough sun right place time year involves early rise tho',\n",
              " 'colbycolberson stop staying night',\n",
              " 'finally synced ipod',\n",
              " 'jess truesdale lmao mess im gonna atl week',\n",
              " 'amalinaaa said getting mcflys live album morrow whats called xx',\n",
              " 'utterhip well catching little piece sharing',\n",
              " 'looking forward daddy returning work saturday gone whole month',\n",
              " 'tomorrow final competition',\n",
              " 'khloekardashian bad catch brazil good luck show amazing',\n",
              " 'lunch forrest baron fpu dead',\n",
              " 'going bed good night everyone love say good morning sweet dream',\n",
              " 'car way home mall got four new adorable item clothing really excited',\n",
              " 'world happiest place denmark finland netherlands',\n",
              " 'morningmajor yep raced round car bike min away dont av much motorsport plane n chopper',\n",
              " 'dreamobscene know feel lovely relaxing weekend',\n",
              " 'm giiggl z lmao ight im dun wit face lmao id rather see ur tho lol',\n",
              " 'magusweaver follow ruin twitter experience',\n",
              " 'yurges loadsa shizze happend im upset',\n",
              " 'rambleredhead think like phoebe mom friend stop movie sad part',\n",
              " 'thisisryanross picture ross',\n",
              " 'fun night tonight house look purrrty tomorrow kid come home',\n",
              " 'iskrin rubbish',\n",
              " 'shutupandsmile ow go back last avatar boyfriend lying bed see',\n",
              " 'last day tv front w colleague sydney paris london',\n",
              " 'dannywood hey danny u run already hope good day love',\n",
              " 'feeling well',\n",
              " 'bpmore last one',\n",
              " 'blakelewis oh see like morning right happy breakfast thumb',\n",
              " 'mitchelmusso wish could call cost lot parent wont let',\n",
              " 'monday mornin back work today good thing live job',\n",
              " 'babyvofficial aww thatz bad ud b great new moon',\n",
              " 'savoy oh naw ill always dha cece chanqes needed ill make',\n",
              " 'trishaanyndhita love adriana',\n",
              " 'ym meebo amp ebuddy really hate u kuhrabbypatty whatsuppp ashpolicarpio cheyennelaxa piaatrinidad',\n",
              " 'mrgarbutt tweet',\n",
              " 'saw credit card statement maybe receive unholy aiden fan package yaaaaay',\n",
              " 'note self wear steel toe court house sayin',\n",
              " 'mikerosenhouse run someone know weekend',\n",
              " 'p liam going look like tool train city hopefully scare old people',\n",
              " 'marko got fight outside roseland tonight rather entertaining',\n",
              " 'woke happy mommy day everyone mom grandma',\n",
              " 'waiting someone sm week nearly month',\n",
              " 'duncan finally got order leaf iraq september open invite stay anytime year gone',\n",
              " 'cape town spitting morning im behind desk sun come perfect timing',\n",
              " 'alanmcnamee nope idea registered far response',\n",
              " 'invited one beach short notice mean hell tell hr really',\n",
              " 'everbrandy look like fun',\n",
              " 'hey write yet little girl middle ride',\n",
              " 'blerg damn monday must always come decent weekend new glass hair color got carded gamestop awesome ego boost',\n",
              " 'daveduarte remember evidence based management lecture definitely best ebm',\n",
              " 'omg seated seth lakeman way far back liking x',\n",
              " 'infidel people ask lot plus thought id contribute sykes trending topic',\n",
              " 'craftedclay name brand dairy even called semi sweet dark try get helloveggies store',\n",
              " 'http twitpic com j om non civilisation pretty though',\n",
              " 'worldarts see approaching nice round number expect crashed time come back tonight',\n",
              " 'vp gone getting headache time coffee fight migraine ohnoyoudidnt',\n",
              " 'billyscallywag load highly qualified stuff load still using snail mail',\n",
              " 'fashion retweet best running skirt scene well',\n",
              " 'got wheres sun',\n",
              " 'laundry loud music relaxing',\n",
              " 'freaking cold sydney tonight',\n",
              " 'trisha dan humphires adorable human ever lt lt lt',\n",
              " 'truly sad cheap little camcorder shot crap',\n",
              " 'waking time hot cup coffee',\n",
              " 'http twitpic com j z know lold',\n",
              " 'sentimentalizzy lady water pure b remember enjoying film',\n",
              " 'trued rim getting good',\n",
              " 'ellapaigebabe would amazing could meet u germany germany twice',\n",
              " 'busy exam week coming always look bright side life whistle',\n",
              " 'watched rescuer dru ate mickey disney sequel done',\n",
              " 'stkulp lol tweet always fun follow never dull moment kulps',\n",
              " 'eogasawa looked great sick yesterday sorry really make effort say hi preoccupied',\n",
              " 'taylorswift woooooooooo coming nottingham point lovelovelove lt',\n",
              " 'oh great tampa people anybody area know someone work jain society tampa bay none phone work',\n",
              " 'jasonbradbury sure suprisingly addictive tho signed month trying resist',\n",
              " 'good old friend ur new job another good day work paycheck day even better',\n",
              " 'hippyofdoom even get chance shop aldi',\n",
              " 'got dailybooth sure confusing',\n",
              " 'still recouperating holiday weekend wow never sunburnt really hurt',\n",
              " 'dow future point night trading look like may good start far f',\n",
              " 'unfortunately heffas decided take impromptu mall trip without telling',\n",
              " 'say miss cousin badly http plurk com p x bha',\n",
              " 'jeremyfritsche understanding would require taking paragraph context para could lead opting every class',\n",
              " 'tommcfly aww bless haha cute tom',\n",
              " 'stevico nice ahd work today',\n",
              " 'burbleon hope dream come true',\n",
              " 'miss mom today best friend even though gone several yr still miss dearly happy mother day',\n",
              " 'tinabby got right know found coming cried really suck im still happy headlining',\n",
              " 'heading xtra vision get part season csi miami baby',\n",
              " 'grandparent place',\n",
              " 'tatianemarks quem katy',\n",
              " 'http tinyurl com c z gn wow wow hope get sort gut near future',\n",
              " 'amaury polanco hi uploaded completely new chinese lesson www youtube com chineselearn please feel free watch enjoy',\n",
              " 'nothing good tv',\n",
              " 'sabrina thought simple one found hilarious',\n",
              " 'ropemarksmuse let guess skipped sport bought new pinkish outfit',\n",
              " 'going check make sure fishies dead poor fishies',\n",
              " 'creyes middle school elem high school remain open need credit graduate cali broken',\n",
              " 'last sunday going home',\n",
              " 'makeup cute dress ready go',\n",
              " 'jmlares actually find something like easy get good gift lot money tbh nowhere near personal though',\n",
              " 'vexdigital still love fact scratch winner',\n",
              " 'mareenshere slept',\n",
              " 'http www bodybuilding com fun richardchan htm omfg dude look well ard put shame',\n",
              " 'timkealey wong',\n",
              " 'say quot happy mother day quot',\n",
              " 'cloud thought thing felt bit better saw le camera',\n",
              " 'althearicardo hr go pa ako',\n",
              " 'everyone follow followmandy followsavvy amazing',\n",
              " 'keel haha agreed lol',\n",
              " 'grr hate damn near forced go place especially sit bus entire day sit house podunktown va',\n",
              " 'lilers least done',\n",
              " 'geoff posted boo coming home tenerife boo http boo fm b',\n",
              " 'ever delete twitter',\n",
              " 'col rftl think pembsdave must blocked heard tweet',\n",
              " 'taking load old broken concrete landfill tried find recycler',\n",
              " 'giangck ph ru cho b ng tr c khi b th c r ng tr c n mua th kg n th ch',\n",
              " 'getting oil change apparently mouse eating birdseed garage also eating air filter',\n",
              " 'thetastydactyls try coat hanger trick',\n",
              " 'need follower get follower fallow fallow back',\n",
              " 'cause old mate hehe',\n",
              " 'zaidah listen music work probably could try start hunting new job monday',\n",
              " 'zomberellamcfox fan',\n",
              " 'twitter spam oh',\n",
              " 'autisminsights laundry instead sitting darn laundry keep mocking',\n",
              " 'hithere heck going',\n",
              " 'dougiemcfly haha dude b date p wish u ur b day oth november also turn u reply please',\n",
              " 'musically headphone awesome staying ear good',\n",
              " 'yuraf nope kidding sometimes think forest gump running year',\n",
              " 'damiencripps hobo come next week buying drink',\n",
              " 'senior done day woohoo going night',\n",
              " 'darealtinat thank u',\n",
              " 'mood long car drive',\n",
              " 'jason bear love name',\n",
              " 'sampad till ice cream',\n",
              " 'mattg didgeridoo',\n",
              " 'want transfer math knowledge head via usb cable',\n",
              " 'think going get see sun today gutted need colour',\n",
              " 'thinking pay end',\n",
              " 'chanterene feel better love bring u soul food make u feel good',\n",
              " 'joolzp studying computing year bring feck changing plant science next year',\n",
              " 'day hour work really like job fabulous b',\n",
              " 'hangover',\n",
              " 'jonaskevin loved guy amazing',\n",
              " 'doesnt hangover getting ready good ol english fry',\n",
              " 'laurenglennon sorry listen bat gd dude never got tell bat amazing lt',\n",
              " 'wish outside thats thing thats gonna kill work day everyday summer',\n",
              " 'sara isenough hahah hilarious b f wash usually fit bed anymore either turn pink',\n",
              " 'thecultureofme feel bad burrito kinda sucked',\n",
              " 'cute dress hyped http lookbook nu look',\n",
              " 'antwoinne hey babe nothing much tryin see imma work today lol look like load isnt bad',\n",
              " 'thanks raspberry laced vanilla cake',\n",
              " 'friend gone sushi movie laughter needed fun',\n",
              " 'listen back around ddlovato love song meet tiniolsen tomorrow go swimming sorry',\n",
              " 'michellem dsl welcome',\n",
              " 'finally friday still grounded till next thursday stereo life lt',\n",
              " 'ok fuckin hungry fat know know aha',\n",
              " 'really wish spare cash buy new punch wii',\n",
              " 'quot threatened patronized anything eat thought connection quot movie night',\n",
              " 'rockeye ahhh follow dark side night',\n",
              " 'igzzie easy trying learn serious normal two new word',\n",
              " 'karen waaa',\n",
              " 'im awesome',\n",
              " 'gialovescece ghost whisperer going without j love',\n",
              " 'simonwilder want play',\n",
              " 'limeice obviously wasnt coz u werent pc fixed',\n",
              " 'feel good today',\n",
              " 'going forgo pub night wife tonight tough week u neither really plus neighbor died',\n",
              " 'watch lowkey freestyling selling mixtape http tinyurl com dev xh performing norwich tonight',\n",
              " 'camera doucheeee stopped working',\n",
              " 'head feel sooooooooooo much better',\n",
              " 'mongab want ya l shoulve gone rm',\n",
              " 'ddlovato ya headin family vaca hope paparazzo leave alone',\n",
              " 'trying upload one thousand photo',\n",
              " 'fatinee haha many time',\n",
              " 'gorgeousapg hahahaha nahh sowwieee git shirt match dez sneakerz nd idk da color aghhh lol',\n",
              " 'got fully denied tonight sleep time lnd morn',\n",
              " 'maxsterism naisee bad see lens flare arond listening xd cant really catch saying tho',\n",
              " 'bx ohh yes rented one thought roomy hubby wont go welp congrats new baby',\n",
              " 'breadesign congratulation chap pas love claude',\n",
              " 'cant use blackberry today suck',\n",
              " 'rockndan dude tried vegetarian thing lasted month good luck going vegan intense good move though',\n",
              " 'last friday highschool intense one test go economics ewww man',\n",
              " 'dutchrudder told em english special quality come cussing welcome fun dutch',\n",
              " 'sunny walk round lake morning met mother goose father goose five baby gosling',\n",
              " 'moyo oh woke accidentally get fall asleep scare p p story jk',\n",
              " 'warm evening met new neighbor nice',\n",
              " 'homee bored plzz someone txt something weekend',\n",
              " 'jason manford awww thats lovely thing u say u manchester x',\n",
              " 'listening wait akon feat pain love song',\n",
              " 'alexalltimelow ohh snapp fun',\n",
              " 'mother day family dinner rainy sunday night',\n",
              " 'whitney good girl whitney',\n",
              " 'happy mother day heidi klum',\n",
              " 'might still eating drinking',\n",
              " 'photo novusnovendo eyecandy live like stripclub keep clothes take http tumblr com x k wga',\n",
              " 'mileycyrus dnt think ever get tired quot climb quot sngs u always remember',\n",
              " 'goodnight twitter ill see hour sleep',\n",
              " 'genehiga studio annhamilton bogie became one best friend guy started',\n",
              " 'woo got fast mobile broadband connection',\n",
              " 'orphanth pasta pizzahut',\n",
              " 'getting butt kicked project idea get perfect',\n",
              " 'nicholina got message twitter freak',\n",
              " 'terrencej got job amp degree damn car died lamborghinibow made feel shitty bout havin car',\n",
              " 'forgot complain southwest preboard family worst part travelling',\n",
              " 'bye people',\n",
              " 'kgthagreat yea tre hood claim thats atl theme song aint lol youtube vid posted yesterday',\n",
              " 'mumbleguy need men give input fill u time soon',\n",
              " 'milan retardeddddddd anyone wanna see paint pic',\n",
              " 'jazzylov loll dad leavingg moving la vega myy ex boyfrienndd finallly done talking left alone',\n",
              " 'fall asleep didnt get see jonas brother web cast still tierd',\n",
              " 'ijanette http twitpic com wl x pretty janette exact hey monday band arm',\n",
              " 'hate tomtom city traffic suck',\n",
              " 'warwick castle http snurl com izzau amazing devon week internet update',\n",
              " 'another lonely night',\n",
              " 'beautiful morning mountain',\n",
              " 'oh god sad day',\n",
              " 'bad day work stressful involved minor accident everything ok far cut immediately',\n",
              " 'davidarchie good luck remember right everything england rip anyway next time write back haha',\n",
              " 'beauabroughton shud lol device thing work',\n",
              " 'matter marries long youre happy find wrong say ok u two marry cant',\n",
              " 'go captain say quot shut quot way http blip fm z p',\n",
              " 'jrknaughtynurse yep thespunkyone b day kiss gonna happen right jonathanrknight',\n",
              " 'nzclothnappy hope georgia still wide awake hiccup',\n",
              " 'russzart ohh noo joshua im sooo sorry im really sorry didnt get see please forgive sorry lt',\n",
              " 'bardicus gotta say little jealous',\n",
              " 'http twitpic com woke morning want leave bed',\n",
              " 'lt alayellow',\n",
              " 'get job',\n",
              " 'right adrianmw',\n",
              " 'abhorrentbm lame said hello',\n",
              " 'buy new washer bought dryer dryer labeled electric gas grrr load tomorrow return',\n",
              " 'mcdaydreamer best friend go school others change school',\n",
              " 'tamij come pup certainly near future maybe beagle depends mus fatboy',\n",
              " 'got need leave class hour',\n",
              " 'strawfoot markn johnhalton seriously guy wanted kick monday starwarswithaddedpants spam really',\n",
              " 'tsunamiwavin lol know',\n",
              " 'already hate tan line oh well better get used',\n",
              " 'landice said quot uhmazing quot uhtterly uhmazing helenuh',\n",
              " 'lharp franto app finally face truth lack time never able achieve goal life',\n",
              " 'sick constant bad dream grr',\n",
              " 'johncmayer lester pitcher red sox',\n",
              " 'work',\n",
              " 'get crazy',\n",
              " 'dropped sistah lax fb',\n",
              " 'training today',\n",
              " 'inluvwithjon managed breathe since getting news',\n",
              " 'may th happy star war day http ow ly aw',\n",
              " 'vachan checked spb mobile application rock gr potential going forward quot must quot window mobile user',\n",
              " 'ep well twitter even work phone anymore',\n",
              " 'magnum knew mom loved flower love rieger begonia hanging basket seemed perfect',\n",
              " 'bored gonna go carnival get day pas tomorrow excited bored today',\n",
              " 'sloanyxxx thanks',\n",
              " 'ordered asus eee pc',\n",
              " 'subject frosty beer bloody fridge conked beer warm',\n",
              " 'nickindarsingh suck',\n",
              " 'watching tv patrol manny news',\n",
              " 'happy mother day mom',\n",
              " 'uhh wish someone would include follow friday would great get follower',\n",
              " 'yay poker win sketchy student decided show today lawl',\n",
              " 'nice seeing partner crime gossip drama queen partner missed manda',\n",
              " 'oxygen aww ty bummer watching conversation',\n",
              " 'going bc making break',\n",
              " 'awe gotta go mom command sleep already',\n",
              " 'planning week going good going get lot stuff done',\n",
              " 'happy mother day mother',\n",
              " 'natashayi garlic pill spider bite toe lol classic always hungry u killing funny',\n",
              " 'bondean sad thing wanted work year dream job crushed saw crappy still bitter',\n",
              " 'ive got mad cramp leg',\n",
              " 'statkallday yessssssir',\n",
              " 'sajal meant lazy work prof',\n",
              " 'mitsuhiko np mate great meeting prague',\n",
              " 'happy mother day',\n",
              " 'hate public internet sometimes even reach farther transmitter sitting right next win',\n",
              " 'changed way golden gaytimes made used awesome',\n",
              " 'process learning drive stick shift getting easier thanks mrjimson',\n",
              " 'gotta get early tomorrow gotta work',\n",
              " 'victoryhawk darn bacon',\n",
              " 'ineska awwww made realize take bulletin board many memory',\n",
              " 'natealo hope great weekend congratulation',\n",
              " 'simonpeggster hi simon new thought tell jimmy carr tweeted make star trek movie',\n",
              " 'home alone friday night doesnt get sadder folk',\n",
              " 'bubble hey nothing wrong',\n",
              " 'mileycyrus ahhhh voting closed category would voted though tried',\n",
              " 'walking teignmouth dawlish http twitpic com jf k',\n",
              " 'love day school gt studyin quietly',\n",
              " 'lol people falcon boo one becca',\n",
              " 'saw thirst korean movie made chan wook park movie hard good',\n",
              " 'im pain shit',\n",
              " 'someone take la need see lemon tree',\n",
              " 'joelyrighteous silly shoot shopper though taser probably humane penalty item item line',\n",
              " 'tagscats thanks know happens awhile',\n",
              " 'nemke stackeoverflow http bit ly nfk',\n",
              " 'bff rocking hotel california nkotb song done stay next hmmm careless whisper',\n",
              " 'woke cleaned aunt emmas walmart commissary time nap work',\n",
              " 'hadoukenuk http twitpic com hsd weheyyyy give thanks midi keyboard',\n",
              " 'jtimberlake snl tonight going hilarious',\n",
              " 'mandacrow pic donnie one siouxsinner took last night launch word got solo album comin',\n",
              " 'jeffmello sale training couple paper paid bill bit',\n",
              " 'sarahismail cool',\n",
              " 'joyousnfree lol new diet plan ran late wrk hav chance make lunch ate lot got hm',\n",
              " 'caitiehendry http www sife org usaexpo',\n",
              " 'taufiqz lol know aim please',\n",
              " 'betterislittle rain pours true sorry hate kinda time',\n",
              " 'musicmonday nicest thing kate nash',\n",
              " 'wikileaks geht wieder super wikileaks online',\n",
              " 'odee early qot call heard lonq time',\n",
              " 'sooo reluctant study final right',\n",
              " 'kaliboooo bucher class cried',\n",
              " 'bust magazine thats dream thats reality c wishlist',\n",
              " 'pshh dont',\n",
              " 'neenz bye great meeting',\n",
              " 'rainy monday better day work home thankful',\n",
              " 'way see grandparent',\n",
              " 'ups ketauan cabut sama gemamalove chikachilo nih',\n",
              " 'jayce kay morning hows day hope another lying sun',\n",
              " 'berinkinsman happen every monday',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zisQgpJPqhgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8RnOqTwSb53z"
      ],
      "authorship_tag": "ABX9TyM6+gkQzVhMJE4CJMxXegmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}